import pandas as pd
import os
import spacy
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import plotly.io as pio
from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora import Dictionary
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

os.environ["TOKENIZERS_PARALLELISM"] = "false"


# ==============================================================================
# 1. CONFIGURA√á√ïES E CONSTANTES
# ==============================================================================

# Caminho para o arquivo de entrada
CAMINHO_CSV = "maritacaPrompt2/poemas_unificados.csv"
# chatGPTPrompt1 googleTradutor maritacaPrompt1

# Pasta para salvar os resultados
PASTA_SAIDA = "maritacaPrompt2/original"

# Coluna do DataFrame a ser utilizada
COLUNA_POEMAS = "original_poem"  # "original_poem", "translated_poem", "translated_by_TA"

# Defini√ß√£o dos idiomas de origem e destino para filtrar o CSV
IDIOMA_ORIGEM = "fr_XX"  #  "fr_XX", "pt_XX", "en_XX"
IDIOMA_DESTINO = "en_XX" #  "fr_XX", "pt_XX", "en_XX"

# Idioma para o pr√©-processamento (NLTK e spaCy)
IDIOMA_PROC = "fr_XX"

nr_topics = 4
# 3 at√© o 7 - qtd de topicos reais +1

# ==============================================================================
# 2. DEFINI√á√ÉO DAS FUN√á√ïES
# ==============================================================================

def carregar_modelo_spacy(idioma: str):
    """Carrega o modelo spaCy apropriado para o idioma especificado."""
    print(f"‚úîÔ∏è Carregando modelo spaCy para o idioma: {idioma}")
    modelos = {
        "pt_XX": "pt_core_news_sm",
        "fr_XX": "fr_core_news_sm",
        "en_XX": "en_core_web_sm"
    }
    if idioma not in modelos:
        raise ValueError(f"Idioma '{idioma}' n√£o suportado pelo spaCy neste script.")
    return spacy.load(modelos[idioma])

def preprocessar_texto(texto: str, nlp_model, stopwords_custom: set, usar_lematizacao=True) -> str:
    """
    Realiza o pr√©-processamento de um texto, incluindo lematiza√ß√£o opcional e remo√ß√£o de stopwords.
    """
    if usar_lematizacao:
        # Lematiza√ß√£o com spaCy
        doc = nlp_model(texto)
        tokens = [
            token.lemma_.lower()
            for token in doc
            if not token.is_stop and not token.is_punct and not token.like_num and len(token.text) > 2
        ]
    else:
        # Apenas tokeniza√ß√£o com NLTK se a lematiza√ß√£o for desativada
        tokens = word_tokenize(texto.lower())

    # Filtragem final para garantir a qualidade dos tokens
    tokens_filtrados = [
        token for token in tokens
        if token.isalpha() and token not in stopwords_custom and len(token) > 2
    ]
    return " ".join(tokens_filtrados)

def salvar_topicos_legiveis(topic_model: BERTopic, caminho_arquivo: str):
    """Salva os t√≥picos e suas palavras-chave em um arquivo de texto."""
    with open(caminho_arquivo, "w", encoding="utf-8") as f:
        for topic_num, palavras in topic_model.get_topics().items():
            # Pula o t√≥pico de outliers, se desejado (opcional)
            # if topic_num == -1:
            #     continue
            
            count = topic_model.get_topic_info(topic_num)["Count"].iloc[0]
            palavras_str = ", ".join([p[0] for p in palavras])
            f.write(f"T√≥pico {topic_num} ({count} poemas): {palavras_str}\n")

# ==============================================================================
# 3. BLOCO DE EXECU√á√ÉO PRINCIPAL
# ==============================================================================

# Este bloco protege o c√≥digo de ser re-executado em processos filhos,
# resolvendo o erro 'RuntimeError' no Windows/macOS.
if __name__ == '__main__':

    # Garante que a pasta de sa√≠da exista
    os.makedirs(PASTA_SAIDA, exist_ok=True)

    # --- Parte 1: Carregamento e Filtragem dos Dados ---
    print(f"üìñ Carregando dados de '{CAMINHO_CSV}'...")
    df = pd.read_csv(CAMINHO_CSV)
    df = df[(df["src_lang"] == IDIOMA_ORIGEM) & (df["tgt_lang"] == IDIOMA_DESTINO)].reset_index(drop=True)
    poemas = df[COLUNA_POEMAS].astype(str).tolist()
    print(f"Encontrados {len(poemas)} poemas para an√°lise.")

    # --- Parte 2: Pr√©-processamento ---
    print("üßπ Iniciando pr√©-processamento dos textos...")
    
    # Mapeia o c√≥digo de idioma para o NLTK
    mapa_idioma_nltk = {"pt_XX": "portuguese", "en_XX": "english", "fr_XX": "french"}
    idioma_nltk = mapa_idioma_nltk[IDIOMA_PROC]

    # Carrega stopwords e adiciona termos personalizados
    stopwords_personalizadas = set(stopwords.words(idioma_nltk))
    if IDIOMA_PROC == "fr_XX":
        stopwords_personalizadas.update(["le", "la", "les", "un", "une", "jean", "john", "kaku", "lorsqu", "jusqu", "sai"])
    elif IDIOMA_PROC == "pt_XX":
        stopwords_personalizadas.update(["o", "a", "os", "as", "um", "uma", "eu", "tu", "ele", "ela", "n√≥s", "v√≥s", "eles", "elas"])
    elif IDIOMA_PROC == "en_XX":
        stopwords_personalizadas.update(["the", "a", "an", "and", "but", "or", "so", "to", "of", "in", "for", "on", "at"])
    
    # Carrega o modelo spaCy UMA √öNICA VEZ para efici√™ncia
    nlp = carregar_modelo_spacy(IDIOMA_PROC)

    # Aplica o pr√©-processamento a todos os poemas
    poemas_limpos = [preprocessar_texto(p, nlp, stopwords_personalizadas) for p in tqdm(poemas, desc="Processando poemas")]

    # --- Parte 3: Gera√ß√£o de Embeddings ---
    print("üîó Carregando modelo de embeddings (SentenceTransformer)...")
    embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    print("üîó Gerando embeddings para os poemas...")
    embeddings = embedding_model.encode(poemas_limpos, show_progress_bar=True)

    # --- Parte 4: Cria√ß√£o e Treinamento do Modelo BERTopic ---
    print("üìö Criando e treinando o modelo BERTopic...")
    topic_model = BERTopic(language="multilingual", nr_topics=nr_topics) 
    topics, _ = topic_model.fit_transform(poemas_limpos, embeddings)

    # --- Parte 5: An√°lise e Salvamento dos Resultados ---
    print("üìä Adicionando t√≥picos ao DataFrame e salvando...")
    df["topic"] = topics
    df.to_csv(f"{PASTA_SAIDA}/poemas_com_topicos.csv", index=False)

    print("üìù Salvando descri√ß√£o dos t√≥picos em TXT...")
    salvar_topicos_legiveis(topic_model, f"{PASTA_SAIDA}/topicos.txt")

    # --- Parte 6: Gera√ß√£o de Visualiza√ß√µes ---
    print("üé® Gerando e salvando gr√°ficos...")
    
    # Gr√°fico de Barras dos T√≥picos
    n_topicos_reais = len(topic_model.get_topic_freq()) - (1 if -1 in topic_model.get_topic_freq().Topic.values else 0)
    fig_bar = topic_model.visualize_barchart(top_n_topics=n_topicos_reais, n_words=10, width=400, height=400)
    pio.write_image(fig_bar, f"{PASTA_SAIDA}/grafico_barras_topicos.png")

    # Nuvem de Palavras Geral
    texto_total = " ".join(poemas_limpos)
    wc = WordCloud(width=1200, height=600, background_color='white', colormap='viridis').generate(texto_total)
    wc.to_file(f"{PASTA_SAIDA}/nuvem_palavras_geral.png")

    # Gr√°fico de Pizza da Distribui√ß√£o dos T√≥picos
    topic_freq = topic_model.get_topic_freq()
    topic_freq = topic_freq[topic_freq.Topic != -1] # Exclui outliers
    plt.figure(figsize=(8, 8))
    plt.pie(
        topic_freq["Count"],
        labels=[f"T√≥pico {i}" for i in topic_freq["Topic"]],
        autopct='%1.1f%%',
        startangle=140
    )
    plt.title("Distribui√ß√£o Percentual dos T√≥picos")
    plt.savefig(f"{PASTA_SAIDA}/grafico_pizza.png")
    plt.close()
    

    # --- Parte 7: C√°lculo de Coer√™ncia dos T√≥picos ---
    print("üßÆ Preparando dados para o c√°lculo de coer√™ncia...")

    # Adicionamos uma barra de progresso para a tokeniza√ß√£o
    documentos_tokenizados = [doc.split() for doc in tqdm(poemas_limpos, desc="   Tokenizando documentos")]

    dicionario = Dictionary(documentos_tokenizados)

    # Adicionamos uma barra de progresso para a cria√ß√£o do corpus Bag-of-Words
    corpus = [dicionario.doc2bow(doc) for doc in tqdm(documentos_tokenizados, desc="   Criando corpus BoW   ")]

    # Esta parte √© r√°pida, n√£o precisa de barra de progresso
    topicos_palavras = []
    for topic_num in sorted(topic_model.get_topics().keys()):
        if topic_num == -1:
            continue
        palavras = [palavra for palavra, _ in topic_model.get_topic(topic_num)]
        topicos_palavras.append(palavras)

    # Inicia o c√°lculo de coer√™ncia (a parte mais demorada)
    print("‚è≥ Calculando a coer√™ncia do modelo... Isso pode levar alguns minutos.")

    coherence_model = CoherenceModel(
        topics=topicos_palavras,
        texts=documentos_tokenizados,
        dictionary=dicionario,
        corpus=corpus,
        coherence='c_v'
    )
    coherence_score = coherence_model.get_coherence()
    print(f"‚úÖ Coer√™ncia do Modelo c_v: {coherence_score:.4f}")

    print("\nüéâ Processo conclu√≠do com sucesso!")
    print(f"\nQuantidade de t√≥picos reais: {nr_topics-1}")
    print(f"Quantidade de t√≥picos nr_topics: {nr_topics}")